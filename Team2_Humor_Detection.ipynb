{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team2_Humor_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_j5MxGHsc-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8247f77-c974-43a6-bfaf-c1708181f3cf"
      },
      "source": [
        "# Sai, Ravali\n",
        "# MOST of this code came straight from ChronoBERT Amy Olex who used the \n",
        "# Fine-Tuning BERT Tutorial by Chris McCormick at\n",
        "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers\n",
        "!pip install utils\n",
        "\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import sklearn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, get_linear_schedule_with_warmup, AutoTokenizer\n",
        "from transformers import AdamW, BertForSequenceClassification, AutoModelForSequenceClassification\n",
        "from transformers import BertModel, BertConfig\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import utils\n",
        "from math import floor\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import seaborn as sns\n",
        "import os\n",
        "import io\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from io import FileIO\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from transformers import EarlyStoppingCallback\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 450 kB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XifGhC4xQL1"
      },
      "source": [
        "#  Get the files from the google drive\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Get train data file\n",
        "file_id = '11-ZRHPVBWMhMdBXEqjBqVPdTvaZOLLxc'  # Training file on the Google Drive\n",
        "downloaded = io.FileIO(\"Humor_Train_Data.csv\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))\n",
        "\n",
        "# Get test data file\n",
        "file_id = '140hmlr6pZhZXKTgnCkMOnMDsjGdgRX1a'  # Training file on the Google Drive\n",
        "downloaded = io.FileIO(\"Humor_CorrectEstimation_Data.csv\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoWa8W7q4KHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe54b455-ab19-4a92-f48c-df2f62158225"
      },
      "source": [
        "# set the hyperparameters\n",
        "batch_size = 32\n",
        "max_length = 256\n",
        "epochs = 10\n",
        "\n",
        "#  load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#  load model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "#  set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7puGRTMyUoL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "214e350e-6b99-49e9-a1fd-bdb0aaf80c5e"
      },
      "source": [
        "# Load the training and test data\n",
        "train_df = pd.read_csv(\"Humor_Train_Data.csv\", delimiter=',', header=None, names=['Humor', 'Text'], )\n",
        "test_df = pd.read_csv(\"Humor_CorrectEstimation_Data.csv\", delimiter=',', header=None, names=['Humor', 'Text'], encoding = 'unicode_escape')\n",
        "\n",
        "# Create sentence and label lists\n",
        "train_instances = train_df.Text.values\n",
        "test_instances = test_df.Text.values\n",
        "\n",
        "#  Conver the labels to numeric values\n",
        "test_df.Humor.replace({\"no\": 0, \"yes\": 1}, inplace=True)\n",
        "train_df.Humor.replace({\"no\": 0, \"yes\": 1}, inplace=True)\n",
        "\n",
        "#  get the label lists\n",
        "train_Humor = train_df.Humor.values\n",
        "test_Humor = test_df.Humor.values\n",
        "\n",
        "train_df.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Humor</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>What do you call a turtle without its shell? d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>5 reasons the 2016 election feels so personal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Humor                                               Text\n",
              "0      0  Joe biden rules out 2020 bid: 'guys, i'm not r...\n",
              "1      0  Watch: darvish gave hitter whiplash with slow ...\n",
              "2      1  What do you call a turtle without its shell? d...\n",
              "3      0      5 reasons the 2016 election feels so personal\n",
              "4      0  Pasco police shot mexican migrant from behind,..."
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xSX6B8MuY48Q",
        "outputId": "ac81255b-167d-407b-b0fd-4e0b7ac34c53"
      },
      "source": [
        "test_df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Humor</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>What kind of cat should you take into the  des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Remember when people used to have to be in sha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Pizza is always good. - everyone we'll see abo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>What's 6 inches long hard, bent, and in my pan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Black teen's response to violence in his commu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Humor                                               Text\n",
              "0      1  What kind of cat should you take into the  des...\n",
              "1      1  Remember when people used to have to be in sha...\n",
              "2      1  Pizza is always good. - everyone we'll see abo...\n",
              "3      1  What's 6 inches long hard, bent, and in my pan...\n",
              "4      0  Black teen's response to violence in his commu..."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8SPcbUiKwZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7443d2a2-2723-4265-8e13-96ccb6fb0e18"
      },
      "source": [
        "#  Set training data\n",
        "print(\"Number of training instances: \" + str(len(train_instances)))\n",
        "    \n",
        "# Get maximum length of sentences\n",
        "max_length = 0\n",
        "# For every sentence...\n",
        "for sent in train_instances:\n",
        "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "  # Update the maximum sentence length.\n",
        "  max_length = max(max_length, len(input_ids))\n",
        "\n",
        "train_inputs = [] #index_tensor\n",
        "train_masks = [] #attention tensor\n",
        "train_text = [] #tokenized text\n",
        "\n",
        "# For every sentence...\n",
        "for sent in train_instances:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "       sent,  # Sentence to encode.\n",
        "       add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "       max_length=max_length,  # Pad & truncate all sentences.\n",
        "       pad_to_max_length=True,\n",
        "       return_attention_mask=True,  # Construct attn. masks.\n",
        "       return_tensors='pt',  # Return pytorch tensors.\n",
        "       )\n",
        "\n",
        "  # Add the encoded sentence to the list.\n",
        "  train_inputs.append(encoded_dict['input_ids'])\n",
        "  train_masks.append(encoded_dict['attention_mask'])\n",
        "  train_text.append(tokenizer.tokenize(tokenizer.decode(encoded_dict['input_ids'].tolist()[0])))\n",
        "\n",
        "\n",
        "#  Convert to tensors\n",
        "train_inputs = torch.stack(train_inputs).squeeze()\n",
        "train_Humor = torch.tensor(train_Humor)\n",
        "train_masks = torch.stack(train_masks).squeeze()\n",
        "\n",
        "#checking on my shapes\n",
        "#print(train_inputs.shape)\n",
        "#print(train_labels.shape)\n",
        "#print(train_masks.shape)\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_Humor)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training instances: 634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9flFjiYWdds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b840840-186e-4cef-fde6-ccfec1a9762e"
      },
      "source": [
        "#  Set testing data\n",
        "print(\"Number of testing instances: \" + str(len(test_instances)))\n",
        "    \n",
        "# Get maximum length of sentences\n",
        "max_length = 0\n",
        "# For every sentence...\n",
        "for sent in test_instances:\n",
        "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "  # Update the maximum sentence length.\n",
        "  max_length = max(max_length, len(input_ids))\n",
        "\n",
        "test_inputs = [] #index_tensor\n",
        "test_masks = [] #attention tensor\n",
        "test_text = [] #tokenized text\n",
        "\n",
        "# For every sentence...\n",
        "for sent in test_instances:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "       sent,  # Sentence to encode.\n",
        "       add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "       max_length=max_length,  # Pad & truncate all sentences.\n",
        "       pad_to_max_length=True,\n",
        "       return_attention_mask=True,  # Construct attn. masks.\n",
        "       return_tensors='pt',  # Return pytorch tensors.\n",
        "       )\n",
        "\n",
        "  # Add the encoded sentence to the list.\n",
        "  test_inputs.append(encoded_dict['input_ids'])\n",
        "  test_masks.append(encoded_dict['attention_mask'])\n",
        "  test_text.append(tokenizer.tokenize(tokenizer.decode(encoded_dict['input_ids'].tolist()[0])))\n",
        "\n",
        "  #  Convert to tensors\n",
        "test_inputs = torch.stack(test_inputs).squeeze()\n",
        "test_Humor = torch.tensor(test_Humor)\n",
        "test_masks = torch.stack(test_masks).squeeze()\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_Humor)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of testing instances: 38757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwNPWMYfcfiU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20df4c1b-613e-4a7b-b113-5487bec8d5a1"
      },
      "source": [
        "#  Train the model\n",
        "\n",
        "# set parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "num_training_steps = len(train_dataloader) * epochs\n",
        "num_warmup_steps = 0\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n",
        "                                                num_training_steps=num_training_steps)\n",
        "\n",
        "train_loss_set = []\n",
        "training_stats = []\n",
        "\n",
        "# trange is a_bert tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  global model\n",
        "  print(\"Epoch: \", _)\n",
        "  print(\"run training\")\n",
        "\n",
        "  #train the model\n",
        "  model.train()\n",
        "       \n",
        "  # Total loss for this epoch.\n",
        "  tl_set = []\n",
        "  total_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    print(\"Step: \" + str(step))\n",
        "    # # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    # Accumulate the training loss over all of the batches so that we can\n",
        "    # calculate the average loss at the end. `loss` is a_bert Tensor containing a_bert\n",
        "    # single value; the `.item()` function just returns the Python value\n",
        "    # from the tensor.\n",
        "    tl_set.append(loss.item())\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters and take a_bert step using the computed gradient\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    total_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "    avg_train_loss = total_loss / nb_tr_steps\n",
        "    print(\"Length: \" + str(len(tl_set)))\n",
        "    print(\"Average total train loss: {}\".format(total_loss / nb_tr_steps))\n",
        "    print(\"Total Loss for this epoch: \" + str(total_loss))\n",
        "    print(\"Number of steps for this epoch: \" + str(nb_tr_steps))\n",
        "        \n",
        "  print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "run training\n",
            "Step: 0\n",
            "Length: 1\n",
            "Average total train loss: 0.6403961181640625\n",
            "Total Loss for this epoch: 0.6403961181640625\n",
            "Number of steps for this epoch: 1\n",
            "Step: 1\n",
            "Length: 2\n",
            "Average total train loss: 0.6720498502254486\n",
            "Total Loss for this epoch: 1.3440997004508972\n",
            "Number of steps for this epoch: 2\n",
            "Step: 2\n",
            "Length: 3\n",
            "Average total train loss: 0.6815834641456604\n",
            "Total Loss for this epoch: 2.044750392436981\n",
            "Number of steps for this epoch: 3\n",
            "Step: 3\n",
            "Length: 4\n",
            "Average total train loss: 0.6974967569112778\n",
            "Total Loss for this epoch: 2.789987027645111\n",
            "Number of steps for this epoch: 4\n",
            "Step: 4\n",
            "Length: 5\n",
            "Average total train loss: 0.6864918947219849\n",
            "Total Loss for this epoch: 3.4324594736099243\n",
            "Number of steps for this epoch: 5\n",
            "Step: 5\n",
            "Length: 6\n",
            "Average total train loss: 0.6997791826725006\n",
            "Total Loss for this epoch: 4.198675096035004\n",
            "Number of steps for this epoch: 6\n",
            "Step: 6\n",
            "Length: 7\n",
            "Average total train loss: 0.6948278631482806\n",
            "Total Loss for this epoch: 4.863795042037964\n",
            "Number of steps for this epoch: 7\n",
            "Step: 7\n",
            "Length: 8\n",
            "Average total train loss: 0.6867095604538918\n",
            "Total Loss for this epoch: 5.493676483631134\n",
            "Number of steps for this epoch: 8\n",
            "Step: 8\n",
            "Length: 9\n",
            "Average total train loss: 0.676015767786238\n",
            "Total Loss for this epoch: 6.084141910076141\n",
            "Number of steps for this epoch: 9\n",
            "Step: 9\n",
            "Length: 10\n",
            "Average total train loss: 0.6639466643333435\n",
            "Total Loss for this epoch: 6.639466643333435\n",
            "Number of steps for this epoch: 10\n",
            "Step: 10\n",
            "Length: 11\n",
            "Average total train loss: 0.6568867076526989\n",
            "Total Loss for this epoch: 7.2257537841796875\n",
            "Number of steps for this epoch: 11\n",
            "Step: 11\n",
            "Length: 12\n",
            "Average total train loss: 0.6541337370872498\n",
            "Total Loss for this epoch: 7.849604845046997\n",
            "Number of steps for this epoch: 12\n",
            "Step: 12\n",
            "Length: 13\n",
            "Average total train loss: 0.6439635891180772\n",
            "Total Loss for this epoch: 8.371526658535004\n",
            "Number of steps for this epoch: 13\n",
            "Step: 13\n",
            "Length: 14\n",
            "Average total train loss: 0.6403294588838305\n",
            "Total Loss for this epoch: 8.964612424373627\n",
            "Number of steps for this epoch: 14\n",
            "Step: 14\n",
            "Length: 15\n",
            "Average total train loss: 0.6349753896395366\n",
            "Total Loss for this epoch: 9.524630844593048\n",
            "Number of steps for this epoch: 15\n",
            "Step: 15\n",
            "Length: 16\n",
            "Average total train loss: 0.6252248454838991\n",
            "Total Loss for this epoch: 10.003597527742386\n",
            "Number of steps for this epoch: 16\n",
            "Step: 16\n",
            "Length: 17\n",
            "Average total train loss: 0.6184675360427183\n",
            "Total Loss for this epoch: 10.513948112726212\n",
            "Number of steps for this epoch: 17\n",
            "Step: 17\n",
            "Length: 18\n",
            "Average total train loss: 0.6073380129204856\n",
            "Total Loss for this epoch: 10.93208423256874\n",
            "Number of steps for this epoch: 18\n",
            "Step: 18\n",
            "Length: 19\n",
            "Average total train loss: 0.5975572545277444\n",
            "Total Loss for this epoch: 11.353587836027145\n",
            "Number of steps for this epoch: 19\n",
            "Step: 19\n",
            "Length: 20\n",
            "Average total train loss: 0.594936691224575\n",
            "Total Loss for this epoch: 11.8987338244915\n",
            "Number of steps for this epoch: 20\n",
            "Step: 20\n",
            "Length: 21\n",
            "Average total train loss: 0.5830056851818448\n",
            "Total Loss for this epoch: 12.24311938881874\n",
            "Number of steps for this epoch: 21\n",
            "Step: 21\n",
            "Length: 22\n",
            "Average total train loss: 0.5739202214912935\n",
            "Total Loss for this epoch: 12.626244872808456\n",
            "Number of steps for this epoch: 22\n",
            "Step: 22\n",
            "Length: 23\n",
            "Average total train loss: 0.5617855074612991\n",
            "Total Loss for this epoch: 12.921066671609879\n",
            "Number of steps for this epoch: 23\n",
            "Step: 23\n",
            "Length: 24\n",
            "Average total train loss: 0.5525876320898533\n",
            "Total Loss for this epoch: 13.262103170156479\n",
            "Number of steps for this epoch: 24\n",
            "Step: 24\n",
            "Length: 25\n",
            "Average total train loss: 0.5469064056873322\n",
            "Total Loss for this epoch: 13.672660142183304\n",
            "Number of steps for this epoch: 25\n",
            "Step: 25\n",
            "Length: 26\n",
            "Average total train loss: 0.5364778511799299\n",
            "Total Loss for this epoch: 13.948424130678177\n",
            "Number of steps for this epoch: 26\n",
            "Step: 26\n",
            "Length: 27\n",
            "Average total train loss: 0.5286587255972403\n",
            "Total Loss for this epoch: 14.273785591125488\n",
            "Number of steps for this epoch: 27\n",
            "Step: 27\n",
            "Length: 28\n",
            "Average total train loss: 0.5193172820976802\n",
            "Total Loss for this epoch: 14.540883898735046\n",
            "Number of steps for this epoch: 28\n",
            "Step: 28\n",
            "Length: 29\n",
            "Average total train loss: 0.5130198361544773\n",
            "Total Loss for this epoch: 14.877575248479843\n",
            "Number of steps for this epoch: 29\n",
            "Step: 29\n",
            "Length: 30\n",
            "Average total train loss: 0.5050726463397344\n",
            "Total Loss for this epoch: 15.152179390192032\n",
            "Number of steps for this epoch: 30\n",
            "Step: 30\n",
            "Length: 31\n",
            "Average total train loss: 0.4962697428080343\n",
            "Total Loss for this epoch: 15.384362027049065\n",
            "Number of steps for this epoch: 31\n",
            "Step: 31\n",
            "Length: 32\n",
            "Average total train loss: 0.48892711056396365\n",
            "Total Loss for this epoch: 15.645667538046837\n",
            "Number of steps for this epoch: 32\n",
            "Step: 32\n",
            "Length: 33\n",
            "Average total train loss: 0.4815877206397779\n",
            "Total Loss for this epoch: 15.892394781112671\n",
            "Number of steps for this epoch: 33\n",
            "Step: 33\n",
            "Length: 34\n",
            "Average total train loss: 0.47921763272846446\n",
            "Total Loss for this epoch: 16.29339951276779\n",
            "Number of steps for this epoch: 34\n",
            "Step: 34\n",
            "Length: 35\n",
            "Average total train loss: 0.4749790242740086\n",
            "Total Loss for this epoch: 16.6242658495903\n",
            "Number of steps for this epoch: 35\n",
            "Step: 35\n",
            "Length: 36\n",
            "Average total train loss: 0.46782151733835536\n",
            "Total Loss for this epoch: 16.841574624180794\n",
            "Number of steps for this epoch: 36\n",
            "Step: 36\n",
            "Length: 37\n",
            "Average total train loss: 0.46456138225826055\n",
            "Total Loss for this epoch: 17.18877114355564\n",
            "Number of steps for this epoch: 37\n",
            "Step: 37\n",
            "Length: 38\n",
            "Average total train loss: 0.4621475005620404\n",
            "Total Loss for this epoch: 17.561605021357536\n",
            "Number of steps for this epoch: 38\n",
            "Step: 38\n",
            "Length: 39\n",
            "Average total train loss: 0.4567305686382147\n",
            "Total Loss for this epoch: 17.812492176890373\n",
            "Number of steps for this epoch: 39\n",
            "Step: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [00:10<00:10, 10.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 40\n",
            "Average total train loss: 0.4499012820422649\n",
            "Total Loss for this epoch: 17.996051281690598\n",
            "Number of steps for this epoch: 40\n",
            "Training complete!\n",
            "Epoch:  1\n",
            "run training\n",
            "Step: 0\n",
            "Length: 1\n",
            "Average total train loss: 0.23664115369319916\n",
            "Total Loss for this epoch: 0.23664115369319916\n",
            "Number of steps for this epoch: 1\n",
            "Step: 1\n",
            "Length: 2\n",
            "Average total train loss: 0.3089156821370125\n",
            "Total Loss for this epoch: 0.617831364274025\n",
            "Number of steps for this epoch: 2\n",
            "Step: 2\n",
            "Length: 3\n",
            "Average total train loss: 0.24768579006195068\n",
            "Total Loss for this epoch: 0.743057370185852\n",
            "Number of steps for this epoch: 3\n",
            "Step: 3\n",
            "Length: 4\n",
            "Average total train loss: 0.2261471301317215\n",
            "Total Loss for this epoch: 0.904588520526886\n",
            "Number of steps for this epoch: 4\n",
            "Step: 4\n",
            "Length: 5\n",
            "Average total train loss: 0.2827116012573242\n",
            "Total Loss for this epoch: 1.413558006286621\n",
            "Number of steps for this epoch: 5\n",
            "Step: 5\n",
            "Length: 6\n",
            "Average total train loss: 0.28164132436116535\n",
            "Total Loss for this epoch: 1.6898479461669922\n",
            "Number of steps for this epoch: 6\n",
            "Step: 6\n",
            "Length: 7\n",
            "Average total train loss: 0.256894479904856\n",
            "Total Loss for this epoch: 1.798261359333992\n",
            "Number of steps for this epoch: 7\n",
            "Step: 7\n",
            "Length: 8\n",
            "Average total train loss: 0.23974860459566116\n",
            "Total Loss for this epoch: 1.9179888367652893\n",
            "Number of steps for this epoch: 8\n",
            "Step: 8\n",
            "Length: 9\n",
            "Average total train loss: 0.22370094392034742\n",
            "Total Loss for this epoch: 2.013308495283127\n",
            "Number of steps for this epoch: 9\n",
            "Step: 9\n",
            "Length: 10\n",
            "Average total train loss: 0.2147539660334587\n",
            "Total Loss for this epoch: 2.147539660334587\n",
            "Number of steps for this epoch: 10\n",
            "Step: 10\n",
            "Length: 11\n",
            "Average total train loss: 0.20763865248723465\n",
            "Total Loss for this epoch: 2.284025177359581\n",
            "Number of steps for this epoch: 11\n",
            "Step: 11\n",
            "Length: 12\n",
            "Average total train loss: 0.20404825235406557\n",
            "Total Loss for this epoch: 2.448579028248787\n",
            "Number of steps for this epoch: 12\n",
            "Step: 12\n",
            "Length: 13\n",
            "Average total train loss: 0.20078198840984932\n",
            "Total Loss for this epoch: 2.610165849328041\n",
            "Number of steps for this epoch: 13\n",
            "Step: 13\n",
            "Length: 14\n",
            "Average total train loss: 0.19484969973564148\n",
            "Total Loss for this epoch: 2.7278957962989807\n",
            "Number of steps for this epoch: 14\n",
            "Step: 14\n",
            "Length: 15\n",
            "Average total train loss: 0.19385742545127868\n",
            "Total Loss for this epoch: 2.9078613817691803\n",
            "Number of steps for this epoch: 15\n",
            "Step: 15\n",
            "Length: 16\n",
            "Average total train loss: 0.191323216073215\n",
            "Total Loss for this epoch: 3.06117145717144\n",
            "Number of steps for this epoch: 16\n",
            "Step: 16\n",
            "Length: 17\n",
            "Average total train loss: 0.1867931305485613\n",
            "Total Loss for this epoch: 3.1754832193255424\n",
            "Number of steps for this epoch: 17\n",
            "Step: 17\n",
            "Length: 18\n",
            "Average total train loss: 0.19326996761891577\n",
            "Total Loss for this epoch: 3.478859417140484\n",
            "Number of steps for this epoch: 18\n",
            "Step: 18\n",
            "Length: 19\n",
            "Average total train loss: 0.19643733336737282\n",
            "Total Loss for this epoch: 3.7323093339800835\n",
            "Number of steps for this epoch: 19\n",
            "Step: 19\n",
            "Length: 20\n",
            "Average total train loss: 0.19220373295247556\n",
            "Total Loss for this epoch: 3.844074659049511\n",
            "Number of steps for this epoch: 20\n",
            "Step: 20\n",
            "Length: 21\n",
            "Average total train loss: 0.19554283895662852\n",
            "Total Loss for this epoch: 4.106399618089199\n",
            "Number of steps for this epoch: 21\n",
            "Step: 21\n",
            "Length: 22\n",
            "Average total train loss: 0.19182112643664534\n",
            "Total Loss for this epoch: 4.220064781606197\n",
            "Number of steps for this epoch: 22\n",
            "Step: 22\n",
            "Length: 23\n",
            "Average total train loss: 0.18815418753934943\n",
            "Total Loss for this epoch: 4.327546313405037\n",
            "Number of steps for this epoch: 23\n",
            "Step: 23\n",
            "Length: 24\n",
            "Average total train loss: 0.185284155420959\n",
            "Total Loss for this epoch: 4.446819730103016\n",
            "Number of steps for this epoch: 24\n",
            "Step: 24\n",
            "Length: 25\n",
            "Average total train loss: 0.18831124454736708\n",
            "Total Loss for this epoch: 4.707781113684177\n",
            "Number of steps for this epoch: 25\n",
            "Step: 25\n",
            "Length: 26\n",
            "Average total train loss: 0.1844977094576909\n",
            "Total Loss for this epoch: 4.796940445899963\n",
            "Number of steps for this epoch: 26\n",
            "Step: 26\n",
            "Length: 27\n",
            "Average total train loss: 0.1972742963720251\n",
            "Total Loss for this epoch: 5.326406002044678\n",
            "Number of steps for this epoch: 27\n",
            "Step: 27\n",
            "Length: 28\n",
            "Average total train loss: 0.20050795802048274\n",
            "Total Loss for this epoch: 5.614222824573517\n",
            "Number of steps for this epoch: 28\n",
            "Step: 28\n",
            "Length: 29\n",
            "Average total train loss: 0.20215354911212263\n",
            "Total Loss for this epoch: 5.862452924251556\n",
            "Number of steps for this epoch: 29\n",
            "Step: 29\n",
            "Length: 30\n",
            "Average total train loss: 0.19834670623143513\n",
            "Total Loss for this epoch: 5.950401186943054\n",
            "Number of steps for this epoch: 30\n",
            "Step: 30\n",
            "Length: 31\n",
            "Average total train loss: 0.19926904670653806\n",
            "Total Loss for this epoch: 6.177340447902679\n",
            "Number of steps for this epoch: 31\n",
            "Step: 31\n",
            "Length: 32\n",
            "Average total train loss: 0.1982429358176887\n",
            "Total Loss for this epoch: 6.3437739461660385\n",
            "Number of steps for this epoch: 32\n",
            "Step: 32\n",
            "Length: 33\n",
            "Average total train loss: 0.19637043864438028\n",
            "Total Loss for this epoch: 6.480224475264549\n",
            "Number of steps for this epoch: 33\n",
            "Step: 33\n",
            "Length: 34\n",
            "Average total train loss: 0.19723608958370545\n",
            "Total Loss for this epoch: 6.706027045845985\n",
            "Number of steps for this epoch: 34\n",
            "Step: 34\n",
            "Length: 35\n",
            "Average total train loss: 0.19871044542108263\n",
            "Total Loss for this epoch: 6.954865589737892\n",
            "Number of steps for this epoch: 35\n",
            "Step: 35\n",
            "Length: 36\n",
            "Average total train loss: 0.2013403106894758\n",
            "Total Loss for this epoch: 7.248251184821129\n",
            "Number of steps for this epoch: 36\n",
            "Step: 36\n",
            "Length: 37\n",
            "Average total train loss: 0.19799753822184898\n",
            "Total Loss for this epoch: 7.325908914208412\n",
            "Number of steps for this epoch: 37\n",
            "Step: 37\n",
            "Length: 38\n",
            "Average total train loss: 0.19837717084508194\n",
            "Total Loss for this epoch: 7.538332492113113\n",
            "Number of steps for this epoch: 38\n",
            "Step: 38\n",
            "Length: 39\n",
            "Average total train loss: 0.19572782249022752\n",
            "Total Loss for this epoch: 7.633385077118874\n",
            "Number of steps for this epoch: 39\n",
            "Step: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [00:20<00:00, 10.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 40\n",
            "Average total train loss: 0.19584701769053936\n",
            "Total Loss for this epoch: 7.833880707621574\n",
            "Number of steps for this epoch: 40\n",
            "Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUPqrJ6uF1GJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a472a8e2-13ae-4c97-eb80-b0a22b2ff985"
      },
      "source": [
        "#  Inference over the test data and print out the accuracy of the model\n",
        "\n",
        "# Tracking variables\n",
        "total_eval_accuracy = 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model_bert not to compute or store gradients, saving memory and speeding up validation\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = outputs.logits\n",
        "    \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  #  flatten out our predictions and labels to calculate the accuracy\n",
        "  pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "  labels_flat = label_ids.flatten()\n",
        "\n",
        "  # b_input_ids - actual sentence 16 labels_flat 16 pred_flat 16\n",
        "\n",
        "  # text, actual_label, pred_label - map\n",
        "  #  calculate accuracy\n",
        "  total_eval_accuracy += np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "  nb_eval_steps += 1\n",
        "\n",
        "  # get predicitons to list\n",
        "  predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(predict_content)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print(\"Testing complete!\")\n",
        "print(\"Accuracy over the test set: {}\".format(total_eval_accuracy / nb_eval_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing complete!\n",
            "Accuracy over the test set: 0.9268933140734628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYh2dm3fPoqp"
      },
      "source": [
        " **Resizing the Test Labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHsryC0BOAJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e8a774-6a99-4b97-abd6-aace6a3c5552"
      },
      "source": [
        "test_label = []\n",
        "for i in true_labels:\n",
        "    for j in i:\n",
        "        test_label.append(j)\n",
        " \n",
        "test_label = np.array(test_label)\n",
        "test_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiNkdJUePy-k"
      },
      "source": [
        "**Resizing the predicted labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ncj2gMROJIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb550c3b-e87f-4355-ab66-d68017d77d8b"
      },
      "source": [
        "pred = []\n",
        "for i in predictions:\n",
        "    for j in i:\n",
        "        pred.append(j)\n",
        " \n",
        "pred = np.array(pred)\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDeEX_DbP2qk"
      },
      "source": [
        "**Confusion_Matrix for Accuracy,precision,recall,f1 score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctef6qo0OLwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee938dad-5215-4ba5-c6fd-2a581a776165"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "Bert_Based = confusion_matrix(test_label, pred)\n",
        "Bert_Based"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17649,  1624],\n",
              "       [ 1208, 18276]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVNH2UEgOSXR"
      },
      "source": [
        "TP1,TN1,FP1,FN1 = Bert_Based[1][1],Bert_Based[0][0],Bert_Based[0][1],Bert_Based[1][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU6M5r6UOak4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f050a37-e130-4595-d9ba-b651034b63c7"
      },
      "source": [
        "Accuracy = (TP1+TN1)/(TP1+FP1+FN1+TN1)\n",
        "Accuracy "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9269293288954253"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X78XquuAOanl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1941bc55-6379-42e7-c755-2127e0a69572"
      },
      "source": [
        "Precision = TP1/(TP1+FP1)\n",
        "Precision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.918391959798995"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dlj8679OaqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7301db-cdf7-4e13-b87a-ace097a4cb5b"
      },
      "source": [
        "Recall = TP1/(TP1+FN1)\n",
        "Recall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9380004105933073"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kdoathmOf-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209598bc-a04b-4c19-d849-2365b011760b"
      },
      "source": [
        "F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
        "F1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9280926264472883"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FriSJZmCOjvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27abe80a-21f7-4a95-af8a-abde55b02ebc"
      },
      "source": [
        "target_names = ['class 0', 'class 1']\n",
        "print(classification_report(test_label, pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0       0.94      0.92      0.93     19273\n",
            "     class 1       0.92      0.94      0.93     19484\n",
            "\n",
            "    accuracy                           0.93     38757\n",
            "   macro avg       0.93      0.93      0.93     38757\n",
            "weighted avg       0.93      0.93      0.93     38757\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61a3e6JxOmk2"
      },
      "source": [
        "**Gaussian NB Based Approach for classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fqoy3RRO4kn"
      },
      "source": [
        "GNB = GaussianNB()\n",
        "gc = GNB.fit(train_inputs, train_Humor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weuu_Dv_O7N3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e1915b-55d7-4503-d0f9-76b250de5fbd"
      },
      "source": [
        "t_input = test_inputs.resize(77514,31)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:493: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:493: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAtc6TjFO7QX"
      },
      "source": [
        "predict = gc.predict(t_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFbNCWyBO7Sl"
      },
      "source": [
        "p = np.resize(predict,(38757))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6q1XO0WO7VE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504cbb2f-721b-4a98-d9bd-de507fe69cce"
      },
      "source": [
        "cm = confusion_matrix(test_Humor, p)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18314,   959],\n",
              "       [18544,   940]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZNhNra0pGGX"
      },
      "source": [
        "TP,TN,FP,FN = cm[1][1],cm[0][0],cm[0][1],cm[1][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyzXNtWloyWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cac4ee-0db4-437e-f350-e0dc8474949f"
      },
      "source": [
        "Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
        "Accuracy "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49678767706478827"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRhUVGlWpTfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3906f7b5-33dc-4bf8-d18f-97f4229528d6"
      },
      "source": [
        "Precision = TP/(TP+FP)\n",
        "Precision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49499736703528174"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyKgKklHpTrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d642ae79-2059-4d98-884b-7f1c5a838c18"
      },
      "source": [
        "Recall = TP/(TP+FN)\n",
        "Recall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04824471361116814"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMJls1fKpT0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a587563-eaf7-4f8e-ace9-294d2e8f16ca"
      },
      "source": [
        "F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
        "F1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08792031052705421"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6owi3niGPyOR"
      },
      "source": [
        "**Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0CS4FWrpUGj"
      },
      "source": [
        "clf=RandomForestClassifier(n_estimators=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oZi6s6QEc3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb35828-bcbc-4958-de15-5c3e736bb005"
      },
      "source": [
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(train_inputs, train_Humor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIGrD5ZqEc7g"
      },
      "source": [
        "y_pred=clf.predict(t_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ-0nrMmL9WX"
      },
      "source": [
        "y_pred = np.resize(y_pred,(38757))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO4K0zy8LrDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668f9a80-858a-4cf2-d53c-d12b683c3bf8"
      },
      "source": [
        "rm = confusion_matrix(test_Humor,y_pred)\n",
        "rm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14316,  4957],\n",
              "       [14618,  4866]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUsK57dYMLXg"
      },
      "source": [
        "TP2,TN2,FP2,FN2 = rm[1][1],rm[0][0],rm[0][1],rm[1][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3zESG_vMLdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c7de09-baa5-4ce1-f1d9-2f3d274e7c88"
      },
      "source": [
        "Accuracy_random_forest = (TP2+TN2)/(TP2+FP2+FN2+TN2)\n",
        "Accuracy_random_forest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4949299481384008"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjoDZrNHMLk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60af0c97-6c63-40eb-c41f-06a331879013"
      },
      "source": [
        "Precision_rf = TP2/(TP2+FP2)\n",
        "Precision_rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4953680138450575"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Cdk3WYMLrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc79bed9-45cc-4cad-d6d3-08af900c179f"
      },
      "source": [
        "Recall_rf = TP2/(TP2+FN2)\n",
        "Recall_rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2497433791829193"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiDUO5EEMnqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49083cd-9ccd-456f-f457-d5196a0572d4"
      },
      "source": [
        "F1_rf = 2*(Recall_rf * Precision_rf) / (Recall_rf + Precision_rf)\n",
        "F1_rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33207083631896817"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}