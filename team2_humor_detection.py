# -*- coding: utf-8 -*-
"""Team2_Humor_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/134pFLJ45--HmToREAzoNcWYZMaKGvtrm
"""

# Sai, Ravali
# MOST of this code came straight from ChronoBERT Amy Olex who used the 
# Fine-Tuning BERT Tutorial by Chris McCormick at
# https://mccormickml.com/2019/07/22/BERT-fine-tuning/



!pip install transformers
!pip install utils

import argparse
import tensorflow as tf
import torch
import sklearn
from torch.utils.data import TensorDataset, DataLoader, RandomSampler
from sklearn.metrics import classification_report
from transformers import BertTokenizer, get_linear_schedule_with_warmup, AutoTokenizer
from transformers import AdamW, BertForSequenceClassification, AutoModelForSequenceClassification
from transformers import BertModel, BertConfig
from tqdm import tqdm, trange
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import utils
from math import floor
from sklearn.metrics import matthews_corrcoef
import seaborn as sns
import os
import io
from google.colab import auth
from googleapiclient.discovery import build
from io import FileIO
from googleapiclient.http import MediaIoBaseDownload
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from transformers import EarlyStoppingCallback
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

#  Get the files from the google drive
auth.authenticate_user()
drive_service = build('drive', 'v3')

# Get train data file
file_id = '11-ZRHPVBWMhMdBXEqjBqVPdTvaZOLLxc'  # Training file on the Google Drive
downloaded = io.FileIO("Humor_Train_Data.csv", 'w')
request = drive_service.files().get_media(fileId=file_id)
downloader = MediaIoBaseDownload(downloaded, request)
done = False
while done is False:
  status, done = downloader.next_chunk()
  print("Download {}%.".format(int(status.progress() * 100)))

# Get test data file
file_id = '140hmlr6pZhZXKTgnCkMOnMDsjGdgRX1a'  # Training file on the Google Drive
downloaded = io.FileIO("Humor_CorrectEstimation_Data.csv", 'w')
request = drive_service.files().get_media(fileId=file_id)
downloader = MediaIoBaseDownload(downloaded, request)
done = False
while done is False:
  status, done = downloader.next_chunk()
  print("Download {}%.".format(int(status.progress() * 100)))

# set the hyperparameters
batch_size = 32
max_length = 256
epochs = 2

#  load tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

#  load model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

#  set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Load the training and test data
train_df = pd.read_csv("Humor_Train_Data.csv", delimiter=',', header=None, names=['Humor', 'Text'], )
test_df = pd.read_csv("Humor_CorrectEstimation_Data.csv", delimiter=',', header=None, names=['Humor', 'Text'], encoding = 'unicode_escape')

# Create sentence and label lists
train_instances = train_df.Text.values
test_instances = test_df.Text.values

#  Conver the labels to numeric values
test_df.Humor.replace({"no": 0, "yes": 1}, inplace=True)
train_df.Humor.replace({"no": 0, "yes": 1}, inplace=True)

#  get the label lists
train_Humor = train_df.Humor.values
test_Humor = test_df.Humor.values

train_df.head()

test_df.head()

#  Set training data
print("Number of training instances: " + str(len(train_instances)))
    
# Get maximum length of sentences
max_length = 0
# For every sentence...
for sent in train_instances:
  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
  input_ids = tokenizer.encode(sent, add_special_tokens=True)
  # Update the maximum sentence length.
  max_length = max(max_length, len(input_ids))

train_inputs = [] #index_tensor
train_masks = [] #attention tensor
train_text = [] #tokenized text

# For every sentence...
for sent in train_instances:
  encoded_dict = tokenizer.encode_plus(
       sent,  # Sentence to encode.
       add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
       max_length=max_length,  # Pad & truncate all sentences.
       pad_to_max_length=True,
       return_attention_mask=True,  # Construct attn. masks.
       return_tensors='pt',  # Return pytorch tensors.
       )

  # Add the encoded sentence to the list.
  train_inputs.append(encoded_dict['input_ids'])
  train_masks.append(encoded_dict['attention_mask'])
  train_text.append(tokenizer.tokenize(tokenizer.decode(encoded_dict['input_ids'].tolist()[0])))


#  Convert to tensors
train_inputs = torch.stack(train_inputs).squeeze()
train_Humor = torch.tensor(train_Humor)
train_masks = torch.stack(train_masks).squeeze()

#checking on my shapes
#print(train_inputs.shape)
#print(train_labels.shape)
#print(train_masks.shape)

train_data = TensorDataset(train_inputs, train_masks, train_Humor)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)

#  Set testing data
print("Number of testing instances: " + str(len(test_instances)))
    
# Get maximum length of sentences
max_length = 0
# For every sentence...
for sent in test_instances:
  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.
  input_ids = tokenizer.encode(sent, add_special_tokens=True)
  # Update the maximum sentence length.
  max_length = max(max_length, len(input_ids))

test_inputs = [] #index_tensor
test_masks = [] #attention tensor
test_text = [] #tokenized text

# For every sentence...
for sent in test_instances:
  encoded_dict = tokenizer.encode_plus(
       sent,  # Sentence to encode.
       add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
       max_length=max_length,  # Pad & truncate all sentences.
       pad_to_max_length=True,
       return_attention_mask=True,  # Construct attn. masks.
       return_tensors='pt',  # Return pytorch tensors.
       )

  # Add the encoded sentence to the list.
  test_inputs.append(encoded_dict['input_ids'])
  test_masks.append(encoded_dict['attention_mask'])
  test_text.append(tokenizer.tokenize(tokenizer.decode(encoded_dict['input_ids'].tolist()[0])))

  #  Convert to tensors
test_inputs = torch.stack(test_inputs).squeeze()
test_Humor = torch.tensor(test_Humor)
test_masks = torch.stack(test_masks).squeeze()

test_data = TensorDataset(test_inputs, test_masks, test_Humor)
test_sampler = RandomSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)

#  Train the model

# set parameters
param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]
num_training_steps = len(train_dataloader) * epochs
num_warmup_steps = 0
optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,
                                                num_training_steps=num_training_steps)

train_loss_set = []
training_stats = []

# trange is a_bert tqdm wrapper around the normal python range
for _ in trange(epochs, desc="Epoch"):
  global model
  print("Epoch: ", _)
  print("run training")

  #train the model
  model.train()
       
  # Total loss for this epoch.
  tl_set = []
  total_loss = 0
  nb_tr_examples, nb_tr_steps = 0, 0

  # Train the data for one epoch
  for step, batch in enumerate(train_dataloader):
    print("Step: " + str(step))
    # # Add batch to GPU
    batch = tuple(t.to(device) for t in batch)

    # Unpack the inputs from our dataloader
    b_input_ids, b_input_mask, b_labels = batch
    
    # Clear out the gradients (by default they accumulate)
    optimizer.zero_grad()
    
    # Forward pass
    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
    loss, logits = outputs[:2]

    # Accumulate the training loss over all of the batches so that we can
    # calculate the average loss at the end. `loss` is a_bert Tensor containing a_bert
    # single value; the `.item()` function just returns the Python value
    # from the tensor.
    tl_set.append(loss.item())

    # Backward pass
    loss.backward()
    
    # Update parameters and take a_bert step using the computed gradient
    optimizer.step()
    scheduler.step()

    # Update tracking variables
    total_loss += loss.item()
    nb_tr_examples += b_input_ids.size(0)
    nb_tr_steps += 1

    avg_train_loss = total_loss / nb_tr_steps
    print("Length: " + str(len(tl_set)))
    print("Average total train loss: {}".format(total_loss / nb_tr_steps))
    print("Total Loss for this epoch: " + str(total_loss))
    print("Number of steps for this epoch: " + str(nb_tr_steps))
        
  print("Training complete!")

#  Inference over the test data and print out the accuracy of the model

# Tracking variables
total_eval_accuracy = 0
nb_eval_steps, nb_eval_examples = 0, 0
predictions, true_labels = [], []

# Evaluate data for one epoch
for batch in test_dataloader:
  # Add batch to GPU
  batch = tuple(t.to(device) for t in batch)
  
  # Unpack the inputs from our dataloader
  b_input_ids, b_input_mask, b_labels = batch
  
  # Telling the model_bert not to compute or store gradients, saving memory and speeding up validation
  with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
      logits = outputs.logits
    
  # Move logits and labels to CPU
  logits = logits.detach().cpu().numpy()
  label_ids = b_labels.to('cpu').numpy()

  #  flatten out our predictions and labels to calculate the accuracy
  pred_flat = np.argmax(logits, axis=1).flatten()
  labels_flat = label_ids.flatten()

  # b_input_ids - actual sentence 16 labels_flat 16 pred_flat 16

  # text, actual_label, pred_label - map
  #  calculate accuracy
  total_eval_accuracy += np.sum(pred_flat == labels_flat) / len(labels_flat)
  nb_eval_steps += 1

  # get predicitons to list
  predict_content = logits.argmax(axis=-1).flatten().tolist()

  # Store predictions and true labels
  predictions.append(predict_content)
  true_labels.append(label_ids)

print("Testing complete!")
print("Accuracy over the test set: {}".format(total_eval_accuracy / nb_eval_steps))

test_label = []
for i in true_labels:
    for j in i:
        test_label.append(j)
 
test_label = np.array(test_label)
test_label

pred = []
for i in predictions:
    for j in i:
        pred.append(j)
 
pred = np.array(pred)
pred

from sklearn.metrics import confusion_matrix
Bert_Based = confusion_matrix(test_label, pred)
Bert_Based

TP1,TN1,FP1,FN1 = Bert_Based[1][1],Bert_Based[0][0],Bert_Based[0][1],Bert_Based[1][0]

Accuracy = (TP1+TN1)/(TP1+FP1+FN1+TN1)
Accuracy

Precision = TP1/(TP1+FP1)
Precision

Recall = TP1/(TP1+FN1)
Recall

F1 = 2*(Recall * Precision) / (Recall + Precision)
F1

target_names = ['class 0', 'class 1']
print(classification_report(test_label, pred, target_names=target_names))

"""**Gaussian NB Based Approach for classification**"""

GNB = GaussianNB()
gc = GNB.fit(train_inputs, train_Humor)

t_input = test_inputs.resize(77514,31)

predict = gc.predict(t_input)

p = np.resize(predict,(38757))

cm = confusion_matrix(test_Humor, p)
cm

TP,TN,FP,FN = cm[1][1],cm[0][0],cm[0][1],cm[1][0]

Accuracy = (TP+TN)/(TP+FP+FN+TN)
Accuracy

Precision = TP/(TP+FP)
Precision

Recall = TP/(TP+FN)
Recall

F1 = 2*(Recall * Precision) / (Recall + Precision)
F1

"""**Random Forest Classifier**"""

clf=RandomForestClassifier(n_estimators=100)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(train_inputs, train_Humor)

y_pred=clf.predict(t_input)

y_pred = np.resize(y_pred,(38757))

rm = confusion_matrix(test_Humor,y_pred)
rm

TP2,TN2,FP2,FN2 = rm[1][1],rm[0][0],rm[0][1],rm[1][0]

Accuracy_random_forest = (TP2+TN2)/(TP2+FP2+FN2+TN2)
Accuracy_random_forest

Precision_rf = TP2/(TP2+FP2)
Precision_rf

Recall_rf = TP2/(TP2+FN2)
Recall_rf

F1_rf = 2*(Recall_rf * Precision_rf) / (Recall_rf + Precision_rf)
F1_rf